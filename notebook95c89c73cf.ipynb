{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef32100",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-22T07:37:23.029326Z",
     "iopub.status.busy": "2025-10-22T07:37:23.029094Z",
     "iopub.status.idle": "2025-10-22T07:40:49.245395Z",
     "shell.execute_reply": "2025-10-22T07:40:49.244537Z"
    },
    "papermill": {
     "duration": 206.220914,
     "end_time": "2025-10-22T07:40:49.246709",
     "exception": false,
     "start_time": "2025-10-22T07:37:23.025795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32000, 3)\n",
      "Test shape: (8000, 2)\n",
      "Vocab size (including OOV): 20001\n",
      "X_train shape: (28800, 200)\n",
      "X_val shape: (3200, 200)\n",
      "X_test shape: (8000, 200)\n",
      "Class counts (neg,pos): 14413 14387\n",
      "pos_weight for BCEWithLogitsLoss: 1.0018072128295898\n",
      "BiLSTMSentiment(\n",
      "  (embedding): Embedding(20002, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 450/450 [00:06<00:00, 73.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — train_loss: 0.6463 | val_loss: 0.6060 | val_acc: 0.6994 | val_macro_f1: 0.6987\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 450/450 [00:05<00:00, 85.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — train_loss: 0.5950 | val_loss: 0.5636 | val_acc: 0.7009 | val_macro_f1: 0.6994\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 450/450 [00:05<00:00, 85.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — train_loss: 0.5162 | val_loss: 0.5360 | val_acc: 0.7300 | val_macro_f1: 0.7300\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 450/450 [00:05<00:00, 84.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — train_loss: 0.4151 | val_loss: 0.5072 | val_acc: 0.7569 | val_macro_f1: 0.7528\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 450/450 [00:05<00:00, 83.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — train_loss: 0.3369 | val_loss: 0.4241 | val_acc: 0.8106 | val_macro_f1: 0.8105\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 450/450 [00:05<00:00, 83.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — train_loss: 0.2588 | val_loss: 0.4133 | val_acc: 0.8222 | val_macro_f1: 0.8221\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 450/450 [00:05<00:00, 82.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 — train_loss: 0.2153 | val_loss: 0.5268 | val_acc: 0.8022 | val_macro_f1: 0.8013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 450/450 [00:05<00:00, 82.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 — train_loss: 0.1805 | val_loss: 0.4442 | val_acc: 0.8306 | val_macro_f1: 0.8306\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 450/450 [00:05<00:00, 81.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 — train_loss: 0.1298 | val_loss: 0.5324 | val_acc: 0.8272 | val_macro_f1: 0.8271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 450/450 [00:05<00:00, 80.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — train_loss: 0.0977 | val_loss: 0.5927 | val_acc: 0.8287 | val_macro_f1: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: 100%|██████████| 450/450 [00:05<00:00, 80.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 — train_loss: 0.1087 | val_loss: 0.5875 | val_acc: 0.8313 | val_macro_f1: 0.8312\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12: 100%|██████████| 450/450 [00:05<00:00, 79.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 — train_loss: 0.0637 | val_loss: 0.6654 | val_acc: 0.8322 | val_macro_f1: 0.8321\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: 100%|██████████| 450/450 [00:05<00:00, 78.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 — train_loss: 0.0518 | val_loss: 0.7590 | val_acc: 0.8375 | val_macro_f1: 0.8375\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14: 100%|██████████| 450/450 [00:05<00:00, 78.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 — train_loss: 0.0484 | val_loss: 0.6825 | val_acc: 0.8309 | val_macro_f1: 0.8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15: 100%|██████████| 450/450 [00:05<00:00, 77.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 — train_loss: 0.0392 | val_loss: 0.8686 | val_acc: 0.8328 | val_macro_f1: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16: 100%|██████████| 450/450 [00:05<00:00, 76.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 — train_loss: 0.0399 | val_loss: 0.8396 | val_acc: 0.8447 | val_macro_f1: 0.8447\n",
      "Saved best model -> best_lstm_pytorch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17: 100%|██████████| 450/450 [00:05<00:00, 75.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 — train_loss: 0.0526 | val_loss: 0.6879 | val_acc: 0.8281 | val_macro_f1: 0.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18: 100%|██████████| 450/450 [00:06<00:00, 74.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 — train_loss: 0.0305 | val_loss: 0.8870 | val_acc: 0.8303 | val_macro_f1: 0.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19: 100%|██████████| 450/450 [00:06<00:00, 74.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 — train_loss: 0.0176 | val_loss: 0.9983 | val_acc: 0.8403 | val_macro_f1: 0.8403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20: 100%|██████████| 450/450 [00:06<00:00, 74.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 — train_loss: 0.0135 | val_loss: 1.1410 | val_acc: 0.8331 | val_macro_f1: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21: 100%|██████████| 450/450 [00:06<00:00, 73.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 — train_loss: 0.0170 | val_loss: 1.0496 | val_acc: 0.8375 | val_macro_f1: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22: 100%|██████████| 450/450 [00:06<00:00, 73.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 — train_loss: 0.0173 | val_loss: 1.0615 | val_acc: 0.8406 | val_macro_f1: 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23: 100%|██████████| 450/450 [00:06<00:00, 72.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 — train_loss: 0.0146 | val_loss: 1.0559 | val_acc: 0.8350 | val_macro_f1: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24: 100%|██████████| 450/450 [00:06<00:00, 73.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 — train_loss: 0.0108 | val_loss: 1.1337 | val_acc: 0.8375 | val_macro_f1: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25: 100%|██████████| 450/450 [00:06<00:00, 74.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 — train_loss: 0.0155 | val_loss: 1.0385 | val_acc: 0.8381 | val_macro_f1: 0.8381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26: 100%|██████████| 450/450 [00:06<00:00, 74.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 — train_loss: 0.0170 | val_loss: 1.0901 | val_acc: 0.8438 | val_macro_f1: 0.8437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27: 100%|██████████| 450/450 [00:06<00:00, 74.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 — train_loss: 0.0129 | val_loss: 1.1021 | val_acc: 0.8319 | val_macro_f1: 0.8317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28: 100%|██████████| 450/450 [00:06<00:00, 74.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 — train_loss: 0.0175 | val_loss: 0.9789 | val_acc: 0.8316 | val_macro_f1: 0.8314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29: 100%|██████████| 450/450 [00:06<00:00, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 — train_loss: 0.0079 | val_loss: 1.1760 | val_acc: 0.8319 | val_macro_f1: 0.8317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30: 100%|██████████| 450/450 [00:06<00:00, 74.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 — train_loss: 0.0091 | val_loss: 1.2395 | val_acc: 0.8325 | val_macro_f1: 0.8322\n",
      "Val Accuracy: 0.8446875\n",
      "Val Macro F1: 0.8446831165449884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      1602\n",
      "           1       0.85      0.84      0.84      1598\n",
      "\n",
      "    accuracy                           0.84      3200\n",
      "   macro avg       0.84      0.84      0.84      3200\n",
      "weighted avg       0.84      0.84      0.84      3200\n",
      "\n",
      "✅ Saved submission_pytorch.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Imports\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Cell 2 — Config / Hyperparameters\n",
    "DATA_DIR = \"/kaggle/input/imdb-movie-review-sentiment-analysis\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cell 3 — Utility: simple tokenizer (Keras-like behavior)\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_counts = Counter()\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_text(text):\n",
    "        text = text.lower()\n",
    "        # remove punctuation (keep basic contractions if desired)\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for t in texts:\n",
    "            t = '' if pd.isna(t) else t\n",
    "            cleaned = self._clean_text(str(t))\n",
    "            tokens = cleaned.split()\n",
    "            self.word_counts.update(tokens)\n",
    "\n",
    "        # most common\n",
    "        most_common = self.word_counts.most_common(self.num_words) if self.num_words else self.word_counts.most_common()\n",
    "        # reserve index=1.. for words; 1 is for most common word\n",
    "        # index 1 reserved for OOV token\n",
    "        self.word_index = {self.oov_token:1}\n",
    "        idx = 2\n",
    "        for w, _ in most_common:\n",
    "            if w == self.oov_token:\n",
    "                continue\n",
    "            if w in self.word_index:\n",
    "                continue\n",
    "            self.word_index[w] = idx\n",
    "            idx += 1\n",
    "        self.index_word = {i:w for w,i in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        seqs = []\n",
    "        for t in texts:\n",
    "            t = '' if pd.isna(t) else t\n",
    "            cleaned = self._clean_text(str(t))\n",
    "            tokens = cleaned.split()\n",
    "            seq = []\n",
    "            for tok in tokens:\n",
    "                if tok in self.word_index:\n",
    "                    seq.append(self.word_index[tok])\n",
    "                else:\n",
    "                    seq.append(self.word_index[self.oov_token])\n",
    "            seqs.append(seq)\n",
    "        return seqs\n",
    "\n",
    "# padding function\n",
    "from typing import List\n",
    "\n",
    "def pad_sequences(sequences: List[List[int]], maxlen: int, padding='post', truncating='post', value=0):\n",
    "    padded = np.full((len(sequences), maxlen), value, dtype=np.int64)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        if len(seq) <= maxlen:\n",
    "            if padding == 'post':\n",
    "                padded[i, :len(seq)] = seq\n",
    "            else:\n",
    "                padded[i, -len(seq):] = seq\n",
    "        else:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:\n",
    "                seq = seq[-maxlen:]\n",
    "            if padding == 'post':\n",
    "                padded[i, :maxlen] = seq\n",
    "            else:\n",
    "                padded[i, :] = seq\n",
    "    return padded\n",
    "\n",
    "# Cell 4 — Load data\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "\n",
    "# train/val split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train['text'].astype(str),\n",
    "    train['label'].astype(int),\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=train['label']\n",
    ")\n",
    "\n",
    "# Cell 5 — Tokenizer + sequences\n",
    "tokenizer = SimpleTokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "print('Vocab size (including OOV):', len(tokenizer.word_index))\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq   = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(test['text'].astype(str))\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_val   = pad_sequences(X_val_seq,   maxlen=MAX_LEN)\n",
    "X_test  = pad_sequences(X_test_seq,  maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_labels.values.astype(np.int64)\n",
    "y_val   = val_labels.values.astype(np.int64)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "# Cell 6 — Dataset & DataLoader\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, inputs, labels=None):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            return self.inputs[idx]\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "val_dataset   = ReviewDataset(X_val, y_val)\n",
    "test_dataset  = ReviewDataset(X_test, None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Cell 7 — Class weight (pos_weight for BCEWithLogitsLoss)\n",
    "counts = np.bincount(y_train)\n",
    "num_pos = counts[1]\n",
    "num_neg = counts[0]\n",
    "pos_weight = torch.tensor([num_neg / (num_pos + 1e-8)], dtype=torch.float).to(DEVICE)\n",
    "print('Class counts (neg,pos):', num_neg, num_pos)\n",
    "print('pos_weight for BCEWithLogitsLoss:', pos_weight.item())\n",
    "\n",
    "# Cell 8 — Model definition\n",
    "class BiLSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.0 if num_layers==1 else dropout\n",
    "        )\n",
    "        lstm_output_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        # pack/pad could be used — here we use fixed-length padded sequences\n",
    "        output, (hn, cn) = self.lstm(emb)\n",
    "        # use last hidden state(s)\n",
    "        if self.lstm.bidirectional:\n",
    "            # concatenate last forward and backward hidden\n",
    "            last_hidden = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            last_hidden = hn[-1,:,:]\n",
    "        logits = self.fc(last_hidden).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "vocab_size = min(MAX_VOCAB_SIZE + 2, len(tokenizer.word_index) + 2)  # +2 for padding index 0 and safety\n",
    "model = BiLSTMSentiment(vocab_size=vocab_size, embed_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                        num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL).to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Cell 9 — Loss, optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Cell 10 — Training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_path = 'best_lstm_pytorch.pth'\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_losses.append(loss.item())\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            val_preds.extend(preds.tolist())\n",
    "            val_trues.extend(yb.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    val_acc = accuracy_score(val_trues, val_preds)\n",
    "    val_f1 = f1_score(val_trues, val_preds, average='macro')\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    print(f\"Epoch {epoch} — train_loss: {avg_train_loss:.4f} | val_loss: {avg_val_loss:.4f} | val_acc: {val_acc:.4f} | val_macro_f1: {val_f1:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print('Saved best model ->', best_model_path)\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Cell 11 — Full validation report\n",
    "model.eval()\n",
    "val_preds = []\n",
    "val_trues = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        val_preds.extend(preds.tolist())\n",
    "        val_trues.extend(yb.numpy().astype(int).tolist())\n",
    "\n",
    "print('Val Accuracy:', accuracy_score(val_trues, val_preds))\n",
    "print('Val Macro F1:', f1_score(val_trues, val_preds, average='macro'))\n",
    "print(classification_report(val_trues, val_preds))\n",
    "\n",
    "# Cell 12 — Predict on test and save submission\n",
    "all_test_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        all_test_preds.extend(preds.tolist())\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': all_test_preds\n",
    "})\n",
    "submission.to_csv('submission_pytorch.csv', index=False)\n",
    "print('✅ Saved submission_pytorch.csv')\n",
    "\n",
    "# Cell 13 — Notes / next steps\n",
    "# - To use pretrained embeddings (GloVe/fastText), load embedding vectors and assign to model.embedding.weight.data\n",
    "# - Consider using PackedSequence + pack_padded_sequence for better performance on variable-length inputs\n",
    "# - Try Transformer / HuggingFace models for stronger baselines\n",
    "# - Tune MAX_LEN, MAX_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, and training schedule\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14179030,
     "sourceId": 118405,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.764797,
   "end_time": "2025-10-22T07:40:51.033810",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-22T07:37:19.269013",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
